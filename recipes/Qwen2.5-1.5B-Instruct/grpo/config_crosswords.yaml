# Model arguments
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
model_revision: main
torch_dtype: bfloat16
attn_implementation: sdpa

# Data training arguments
dataset_name: od2961/Guardian-cryptonite-official-split 
dataset_prompt_column: problem
dataset_solution_column: answer 
dataset_train_split: train
dataset_validation_split: validation
dataset_test_split: test
system_prompt: |-
  You are an expert *cryptic-crossword solver*. 
  ------------------------------------------------------------
  Every time you receive a clue you must:
  • Analyse it thoroughly.  
  – Pinpoint the **definition**.  
  – Pinpoint the **word-play** (anagram, container, reversal, homophone, charade, etc.).  
  – Write out the full derivation that turns the word-play into the answer.  

  • Check that the answer’s length matches the enumeration in brackets.

  • Respond in **exactly** the tag-based format shown below – no greeting, no commentary outside the tags.  
  – Thinking goes between <think></think>
  The final answer goes inside `<answer>` in UPPER-CASE.  
  – Never reveal the answer elsewhere.

  EXAMPLE: 
  Clue: Pay attention as “SILENT” is rearranged (6)

  Response:
  <think> 1) Quoted bits of the clue: “Pay attention as ‘SILENT’ is rearranged (6)”.
  Device(s): Anagram. Indicator = “rearranged”; fodder = SILENT.
  Derivation:
  Letters available: S I L E N T.
  Rearrange to form a verb meaning “pay attention”.
  Candidate found that matches the definition and uses each letter exactly once.
  Enumeration check: candidate length = 6 ✔.
  Cross-check: definition (“Pay attention”) aligns with the candidate’s meaning ✔.
  No contradictions detected; proceed with final.
  </think> 
  <answer> LISTEN </answer>





# GRPO trainer config
bf16: true
use_vllm: true
do_eval: true
gradient_accumulation_steps: 64
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: od2961/Qwen2.5-1.5B-Open-R1-GRPO-Crosswords-v01
hub_strategy: every_save
learning_rate: 5e-6
adam_beta2: 0.99
adam_epsilon: 1e-06
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine
max_reason_tokens: 150          # keep whole sample ≤ ~1800 tokens
return_reason: true   
max_prompt_length: 1000
max_completion_length: 225
max_steps: -1
num_generations: 4
num_train_epochs: 5
output_dir: data/Qwen2.5-1.5B-Open-R1-GRPO-Crosswords-v01
overwrite_output_dir: true
per_device_eval_batch_size:  8
per_device_train_batch_size: 8
push_to_hub: false
report_to:
- wandb
reward_funcs:
- pure_accuracy
reward_weights:
  - 1.0     # pure_accuracy
kl_target:        0.07       # was 0.05 → aim for ≈40 % of previous KL
init_kl_coeff:    3.00        # was 0.10 → 2× stronger penalty out of the gate
kl_horizon:       50000    # was 100 000 → controller adapts in ~½ the steps
kl_ctl_step_size: 0.15      # was 0.05 → gentler per‑update β adjustments
max_grad_norm: 0.15
vf_coef: 0.25
advantage_normalization: running 
horizon: 1024      
save_strategy: steps
save_steps: 50
evaluation_strategy: steps
eval_steps: 100
save_total_limit: 1000
seed: 42
warmup_ratio: 0.2
reward_normalization: true
gamma: 0.99
gae_lambda: 0.95
log_grad_norm_after_clip: true
log_kl_per_token: true    
reward_clip: [-6.0, 6.0]          
advantage_clip: [-2.5, 2.5]       
clip_range:     0.05          # keep or lower to 0.08
clip_range_vf:  0.5           # tighten critic clipping
callbacks:
  - replay_buffer_callback
  - caching_callback
  - push_to_hub_revision
