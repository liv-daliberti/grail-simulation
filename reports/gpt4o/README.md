# GPT-4o Report Catalog

Outputs generated by `python -m gpt4o.pipeline` live here. The pipeline downloads
the cleaned dataset defined in `src/gpt4o/config.py`, runs a sweep over
temperature / top-p / max-token settings, promotes the best configuration, and writes
both the summary Markdown and the underlying metrics JSON to `reports/gpt4o/`
and `models/gpt-4o/`. Each evaluation stores its prompts, responses, and metrics
under `models/gpt-4o/<temperature>_<tok>_<tp>/` so hyper-parameter combinations
never overwrite one another.

Need the provenance for the cleaned rows? See `clean_data/sessions/README.md`
for the session-ingestion reference.

- `next_video/` – overall accuracy, formatting/parse rates, and fairness cuts by
  issue and participant study for the promoted configuration. The directory also
  captures the raw `metrics.json` emitted by the evaluation stage.
- `opinion/` – change-of-opinion metrics evaluated per participant study, including
  MAE/RMSE, direction accuracy, and calibration summaries plus the corresponding
  prediction traces.
- `hyperparameter_tuning/` – leaderboard of the sweep runs, including the
  deployment parameters and validation metrics (validation split only) that
  drove the selection.

Regenerate everything after updating credentials, prompt templates, or the
underlying dataset:

```bash
PYTHONPATH=src python -m gpt4o.pipeline \
  --out-dir models/gpt-4o \
  --reports-dir reports/gpt4o
```

Use `--stage sweeps` to rerun only the sweep, `--stage finalize` to evaluate the
selected configuration, and `--stage reports` to rebuild the Markdown from
pre-existing artifacts. The SLURM helper `training/training-gpt4o.sh` wires the
same stages for cluster jobs.
