# ===== Model =====
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
torch_dtype: bfloat16
attn_implementation: sdpa

# ===== Data (HUB) =====
dataset_name: od2961/grail-wage     # <— replace with your repo id
dataset_train_split: train
dataset_test_split:  validation
dataset_prompt_column: state_text
dataset_solution_column: next_video_id
system_prompt: |
  You are choosing EXACTLY ONE item from the list of OPTIONS for a specific VIEWER.

  Input you will see:
    • Viewer profile and optional context/history
    • An "OPTIONS:" list with items numbered 1..N
      – Each item is shown as either a title (preferred) or an id

  Your job:
    • Think briefly in <think>…</think> using the viewer’s profile, context, and options.
    • Compare the top 2–3 candidates, then choose the single best option.
    • Never invent new items; choose only from the given OPTIONS list.

  Output format (STRICT):
    - Always include these tags in order: <think>…</think>, <answer>…</answer>, <opinion>…</opinion>.
    - First output your hidden reasoning in <think>…</think>.
      - Reference candidates by their numbers and names (or ids) to justify the choice.
    - Next output ONLY the chosen option’s NUMBER inside <answer>…</answer>.
      - Do NOT output the name, id, or any extra text—ONLY the number.
      - Do NOT include punctuation, quotes, or a period after the number.
    - Finally output <opinion>…</opinion> with exactly one of: increase, decrease, or no_change.

  Example of valid output:
    <think>
    WHY YOU THINK THIS IS THE RIGHT CHOICE
    </think>
    <answer>
    3
    </answer>
    <opinion>
    increase
    </opinion>

  INVALID patterns (never do these):
    <answer>3.</answer>                 ← trailing period
    <answer>"3"</answer>                ← quoted
    <answer>Option 3</answer>           ← extra words
    <answer>Parkland …</answer>         ← name instead of number
    <opinion>maybe</opinion>            ← not one of the allowed direction tokens

max_prompt_length:     2024
max_completion_length: 512

# ===== GRPO trainer =====
bf16: true
use_vllm: true
do_eval: true
per_device_train_batch_size: 8
per_device_eval_batch_size:  8
gradient_accumulation_steps: 16
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

learning_rate: 5e-5
adam_beta2: 0.99
adam_epsilon: 1e-06
lr_scheduler_type: cosine
warmup_ratio: 0.1

num_generations: 4
num_train_epochs: 10
max_steps: -1

output_dir: models/Qwen2.5-1.5B-OpenR1-GRPO
save_strategy: steps
save_steps: 10
save_total_limit: 500

evaluation_strategy: steps
eval_steps: 10

logging_strategy: steps
logging_steps: 1
log_level: info
log_completions: true
report_to: [wandb]

# ===== Rewards =====
reward_funcs:
  - pure_accuracy_reward
reward_weights:
  - 1.0
# ===== KL / PPO knobs =====
kl_target:        0.07
init_kl_coeff:    0.2
kl_horizon:       50000
kl_ctl_step_size: 0.15
clip_range:       0.05
clip_range_vf:    0.5
max_grad_norm:    0.15
vf_coef:          0.25
reward_normalization: true
gamma:     0.99
gae_lambda: 0.95
advantage_normalization: running
reward_clip:     [0, 1]
advantage_clip:  [-1, 1]
log_kl_per_token: true
log_grad_norm_after_clip: true

# ===== Repro / hub =====
seed: 42
push_to_hub: true
hub_model_id: od2961/Qwen2.5-1.5B-OpenR1-GRPO
hub_strategy: every_save
